# Arithmetic Encoding in MoStegoLLM

This document explains how the arithmetic coding implementation in MoStegoLLM
works — how secret data is embedded into LLM-generated text and extracted back.

## Overview

MoStegoLLM implements **LLM-based steganography**: hiding secret binary data
inside natural-looking text generated by a language model. The core mechanism is
arithmetic coding, but used in a non-traditional way. Instead of compressing
text, it uses the LLM's token probability distributions as a shared codebook
between sender and receiver to embed and extract secret bits.

## The Key Insight

At each generation step, an LLM assigns probabilities to its entire vocabulary
of possible next tokens. Arithmetic coding subdivides a numerical interval
`[0, 2^32)` proportionally among the top-k tokens. The secret data bits
determine *where* in that interval we point, which in turn determines *which
token gets selected*. The receiver, running the same model with the same prompt,
reconstructs the same probability distributions and reads the bits back out.

## Architecture (4 core files)

| File | Role |
|------|------|
| `src/mostegollm/utils.py` | Header format, bit-stream conversion, exceptions |
| `src/mostegollm/encoder.py` | Embeds bits into text (arithmetic **decoder** side) |
| `src/mostegollm/decoder.py` | Extracts bits from text (arithmetic **encoder** side) |
| `src/mostegollm/codec.py` | Public API (`StegoCodec` class) |

The naming seems backwards but is accurate: the stego *encoder* is an
arithmetic *decoder* (it reads bits and produces symbols), and the stego
*decoder* is an arithmetic *encoder* (it reads symbols and produces bits).

## Constants

```python
PRECISION = 32
WHOLE = 1 << 32    # 4,294,967,296 — the full interval
HALF  = 1 << 31    # midpoint
QUARTER = 1 << 30  # quarter point
TOP_K = 256        # number of candidate tokens per step
```

The 32-bit precision means the coding interval `[low, high)` always fits in
standard integer arithmetic.

## Token Distribution (`encoder.py:_get_token_distribution`)

At each step, this function runs the LLM forward pass and builds an
integer-scaled cumulative distribution:

1. Get logits from the model's last position, apply temperature.
2. Take the top-k tokens by probability (default 256).
3. Renormalize so they sum to 1.
4. Scale each probability to the integer range `[0, WHOLE)` and compute
   cumulative sums — these are the interval boundaries for arithmetic coding.
5. Fix edge cases: ensure no zero-width intervals (every token gets at least
   width 1), and clamp the final entry to exactly `WHOLE`.

The result is two parallel lists: `token_ids` and `cum_probs`, where token `j`
owns the interval `[cum_probs[j], cum_probs[j+1])`.

## Encoding: Embedding Bits into Text

The encoder (`encoder.py:encode`) works as an arithmetic *decoder* — it reads
bits and outputs symbols (tokens):

### 1. Prepare the bit stream

A 6-byte header (2-byte magic `0x53 0x54` + 4-byte big-endian payload length)
is prepended to the secret data, then the whole thing is converted to a flat
bit list (MSB first per byte).

### 2. Initialize the value register

The first 32 secret bits are read into a `value` register. This register acts
as a "pointer" into the coding interval that drives token selection.

### 3. Main loop (per-token)

For each token position:

- Get the top-k distribution from the LLM.
- Compute `range_size = high - low`.
- **Find the token whose interval contains `value`**: iterate through tokens,
  computing each token's upper boundary as
  `low + (range_size * cum_probs[j+1]) // WHOLE`. The first token where
  `value < sym_high` is the chosen token.
- **Narrow the interval**: set `low` and `high` to the chosen token's
  sub-interval.
- **Renormalize**: when the MSBs of `low` and `high` agree, they carry no more
  information, so shift them out and read new secret bits into `value`:
  - `high <= HALF`: both in lower half — shift out a 0-bit.
  - `low >= HALF`: both in upper half — shift out a 1-bit, subtract HALF.
  - `low >= QUARTER and high <= 3*QUARTER`: straddling the middle — E3
    scaling / underflow prevention. Subtract QUARTER to zoom in on the center.
  - In all three cases: left-shift `low` and `high`, and shift the next secret
    bit into `value`.
- Append the chosen token to the output.

### 4. Termination

The loop runs until `bit_pos > total_bits + PRECISION`, meaning all payload
bits plus 32 padding zero-bits have been consumed. This ensures the decoder can
fully reconstruct the stream.

## Decoding: Extracting Bits from Text

The decoder (`decoder.py:decode`) works as an arithmetic *encoder* — it reads
symbols and outputs bits:

### 1. Set up

Tokenize the cover text and replay the same prompt context the encoder used.

### 2. Main loop (per-token)

For each token in the cover text:

- Reconstruct the same distribution the encoder saw (same model, same prompt,
  same accumulated context).
- **Look up the token**: find its index `j` in the top-k list. If it's not
  found, the text was corrupted or the wrong model/prompt was used.
- **Narrow the interval**: identically to the encoder.
- **Renormalize and emit bits**:
  - `high <= HALF`: emit `0`, then `pending` number of `1`-bits.
  - `low >= HALF`: emit `1`, then `pending` number of `0`-bits. Subtract HALF.
  - `low >= QUARTER and high <= 3*QUARTER`: increment `pending` (deferred
    output for the E3/straddle case). Subtract QUARTER.
  - In all cases: left-shift `low` and `high`.

### 3. Flush

After processing all tokens, emit one final disambiguating bit (0 or 1
depending on whether `low < QUARTER`) plus any remaining `pending` bits.

### 4. Parse the result

The extracted bit stream starts with the 48-bit header. Read the magic bytes
and payload length, then slice out exactly that many payload bits and convert
back to bytes.

## The `pending` Mechanism (E3 Scaling)

This is the subtlest part of the implementation. When `low` is just below the
midpoint and `high` is just above it, neither a `0` nor a `1` can be emitted
yet — the final bit depends on future narrowing. The `pending` counter tracks
how many times this straddle condition occurs consecutively. Once the interval
finally resolves to one side, the deferred bits are emitted as the *opposite*
of the resolving bit (e.g., if `1` resolves, emit `1` followed by `pending`
zeros).

On the encoder side, the corresponding operation subtracts `QUARTER` from
`value` to keep the registers in sync with the decoder's arithmetic.

## Data Flow

```
Secret bytes
    │
    ▼
[Header: magic + length] ++ [payload bytes]
    │
    ▼
Bit stream (MSB-first)
    │
    ▼
Arithmetic decoder (encoder.py)
    ├── reads bits into `value` register
    ├── at each step, LLM provides token distribution
    ├── `value` selects which token's interval it falls in
    └── emits that token → natural text
    │
    ▼
Cover text (looks like normal English)
    │
    ▼
Arithmetic encoder (decoder.py)
    ├── re-tokenizes cover text
    ├── replays same LLM distributions
    ├── looks up each token's interval
    └── emits the bits that interval encodes
    │
    ▼
Bit stream → header + payload → original secret bytes
```

## Why This Works

The encoder and decoder share a **deterministic codebook** — the LLM's
probability distribution at every step. Given the same model, tokenizer, prompt,
`top_k`, and temperature, both sides compute identical interval boundaries. The
encoder uses the secret bits to navigate those intervals (choosing tokens), and
the decoder reverses the process (observing tokens to recover bits).

The information capacity per token is roughly `log2(top_k)` bits, weighted by
the entropy of the distribution — more uniform distributions carry more bits
per token.
